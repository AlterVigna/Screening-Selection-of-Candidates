{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7c6f6c",
   "metadata": {},
   "source": [
    "# Clustering utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd73acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "from random import sample\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Python Implementation for computing Hopkins' Statistic:\n",
    "\n",
    "# function to compute hopkins's statistic for the dataframe X\n",
    "def hopkins_statistic(X):\n",
    "    \n",
    "    X=X.values  #convert dataframe to a numpy array\n",
    "    sample_size = int(X.shape[0]*0.05) #0.05 (5%) based on paper by Lawson and Jures\n",
    "    \n",
    "    \n",
    "    #a uniform random sample in the original data space\n",
    "    X_uniform_random_sample = uniform(X.min(axis=0), X.max(axis=0) ,(sample_size , X.shape[1]))\n",
    "    \n",
    "    #a random sample of size sample_size from the original data X\n",
    "    random_indices=sample(range(0, X.shape[0], 1), sample_size)\n",
    "    X_sample = X[random_indices]\n",
    "   \n",
    "    \n",
    "    #initialise unsupervised learner for implementing neighbor searches\n",
    "    neigh = NearestNeighbors(n_neighbors=2)\n",
    "    nbrs=neigh.fit(X)\n",
    "    \n",
    "    #u_distances = nearest neighbour distances from uniform random sample\n",
    "    u_distances , u_indices = nbrs.kneighbors(X_uniform_random_sample , n_neighbors=2)\n",
    "    u_distances = u_distances[: , 0] #distance to the first (nearest) neighbour\n",
    "    \n",
    "    #w_distances = nearest neighbour distances from a sample of points from original data X\n",
    "    w_distances , w_indices = nbrs.kneighbors(X_sample , n_neighbors=2)\n",
    "    #distance to the second nearest neighbour (as the first neighbour will be the point itself, with distance = 0)\n",
    "    w_distances = w_distances[: , 1]\n",
    "    \n",
    " \n",
    "    \n",
    "    u_sum = np.sum(u_distances)\n",
    "    w_sum = np.sum(w_distances)\n",
    "    \n",
    "    #compute and return hopkins' statistic\n",
    "    H = u_sum/ (u_sum + w_sum)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab94558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility to sort cluster basing on the number of points contained.\n",
    "def orderClusterDescending(numpyArray):\n",
    "    \n",
    "    unique_elements, counts = np.unique(numpyArray, return_counts=True)\n",
    "    \n",
    "    # Create a dictionary to store the counts by element\n",
    "    count_dict = dict(zip(unique_elements, counts))\n",
    "    \n",
    "    # Sort the dictionary by values and get the sorted keys\n",
    "    sorted_keys = sorted(count_dict, key=count_dict.get,reverse=True)\n",
    "\n",
    "    # Create a list of indexes representing the sorted order\n",
    "    indexes = [list(count_dict.keys()).index(key) for key in sorted_keys] \n",
    "    \n",
    "    #for element, count in count_dict.items():\n",
    "\n",
    "    copy2=numpyArray.copy()\n",
    "\n",
    "    for i in range(0,len(numpyArray)-1):\n",
    "        elem=numpyArray[i]\n",
    "        copy2[i]=indexes.index(elem)\n",
    "    \n",
    "    return copy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcb499",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d838882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of elbow method to inspect which number of k is better to select for k-means algoritm\n",
    "\n",
    "def optimise_k_means(data, max_k):\n",
    "    means=[]\n",
    "    inertias=[]\n",
    "    \n",
    "    for k in range(1,max_k):\n",
    "        kmeans=KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        means.append(k)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "    #Generate the elbow plot \n",
    "    fig=plt.subplots(figsize=(10,5))\n",
    "    plt.plot(means,inertias,'o-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd9a25",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315aca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the method of the knee to discover what is the best value for eps.\n",
    "\n",
    "def find_opt_eps(data):\n",
    "\n",
    "    # n_neighbors = 5 as kneighbors function returns distance of point to itself (i.e. first column will be zeros) \n",
    "    nbrs = NearestNeighbors(n_neighbors = 4).fit(data.iloc[:, :])\n",
    "    # Find the k-neighbors of a point\n",
    "    neigh_dist, neigh_ind = nbrs.kneighbors(data.iloc[:, :])\n",
    "    # sort the neighbor distances (lengths to points) in ascending order\n",
    "    # axis = 0 represents sort along first axis i.e. sort along row\n",
    "    sort_neigh_dist = np.sort(neigh_dist, axis = 0)\n",
    "    \n",
    "    k_dist = sort_neigh_dist[:, 3]\n",
    "    plt.plot(k_dist)\n",
    "    plt.ylabel(\"k-NN distance\")\n",
    "    plt.xlabel(\"Sorted observations (4th NN)\")\n",
    "    plt.show()\n",
    "    \n",
    "    kneedle = KneeLocator(x = range(1, len(neigh_dist)+1), y = k_dist, S = 1.0, \n",
    "                      curve = \"concave\", direction = \"increasing\", online=True)\n",
    "\n",
    "    # get the estimate of knee point\n",
    "    #print(kneedle.knee_y)\n",
    "\n",
    "    kneedle.plot_knee()\n",
    "    plt.show()\n",
    "    return kneedle.knee_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a03f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimizing the silhutette index given a range of eps and min_points\n",
    "def gridSearchOptCluster(data,list_eps,list_minPoints):\n",
    "    \n",
    "    best_eps=-1\n",
    "    best_minPoints=-1\n",
    "    best_labels=[]\n",
    "    max_siluette=-1\n",
    "    \n",
    "    for minPoints in list_minPoints:\n",
    "       \n",
    "        for eps in list_eps:\n",
    "            #print (\"minPoints nr : \"+str(minPoints)+\" eps: \"+str(eps))\n",
    "            clusters = DBSCAN(eps = eps, min_samples = minPoints).fit(data.iloc[:, :])\n",
    "            score = silhouette_score(data.iloc[:, :], clusters.labels_, metric='euclidean')\n",
    "            if (score>max_siluette):\n",
    "                max_siluette=score\n",
    "                best_labels=clusters.labels_\n",
    "                best_eps=eps\n",
    "                best_minPoints=minPoints\n",
    "                #print (\"Best score found: \"+str(score)+\" eps: \"+str(eps)+\" minPoints: \"+str(minPoints))\n",
    "    return {\"best_score\": max_siluette,\"best_eps\": eps, \"best_minPoints\":best_minPoints}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
